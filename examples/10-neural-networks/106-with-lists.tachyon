// this program runs a layered classification network that identifies whether a fruit is poisonous
// by relating this value to two properties of the fruit: the size of its spots and the length of
// its spikes.
//
// this is the same as 105-network-with-gradient-descent.tachyon, but with lists instead of
// hardcoded weight/bias values.

cost = fn (weights, biases, layer_data, layer_sizes, layer_amount, print) {
  total_cost = 0;
  x = 0;
  y = 0;

  while y < 30 {
    while x < 30 {
      layer_data = [x, y; 0, 0; 0, 0; 0, 0; 0, 0; 0, 0]

      i = 1
      while i <= layer_amount {
        j = 1
        while j <= layer_sizes[1, i+1] {
          // sum all prev. data * weights + bias
          total = 0
          k = 1
          while k <= layer_sizes[1, i] {
            total = total + layer_data[i, k] * weights[k, (i - 1) * 2 + j]
            k = k + 1
          }
          total = total + biases[j, (i - 1) * 2 + j]

          layer_data[i+1, j] = 1 / (1 + 2.718 ^ (0-total));
          j = j + 1
        }
        i = i + 1
      }

      // some hacks to get the cost value, left side and top side are 1 rest is 0
      expectedVal = 0
      input1copy = x
      while (input1copy < 5) {
        expectedVal = 1
        input1copy = 999
      }
      input2copy = y
      while (input2copy < 5) {
        expectedVal = 1
        input2copy = 999
      }

      print_copy = print
      while print_copy == 1 {
        output1copy = layer_data[layer_amount+1, 1]
        output2copy = layer_data[layer_amount+1, 2]
        while (output1copy >= output2copy) {
          print("S ")
          output1copy = 0-9999
        }
        condn = 0-9999
        while (output1copy != condn) {
          print("F ")
          output1copy = 0-9999;
        }
        print_copy = 0
      }

      val1 = (layer_data[layer_amount+1, 1] - expectedVal) ^ 2
      val2 = (layer_data[layer_amount+1, 2] - (1-expectedVal)) ^ 2
      total_cost = total_cost + val1 + val2
      x = x + 1
    }
    print_copy = print
    while print_copy == 1 {
      print("\n")
      print_copy = 0
    }
    x = 0
    y = y + 1
  }

  return total_cost
}

// some weights and biases to start with
weights = [
  1.0, 0-1.0,  1.0, 0-1.0,  1.0, 0-1.0,  1.0, 0-1.0,  1.0, 0-1.0;
  0-1.0, 1.0, 0-1.0,  1.0, 0-1.0,  1.0, 0-1.0,  1.0, 0-1.0,  1.0]
biases = [
  0-5.0, 0-5.0, 0, 0, 0, 0, 0, 0, 0, 0;
  0-5.0, 0-5.0, 0, 0, 0, 0, 0, 0, 0, 0]
// start with just the input data, and the shadow of the real data. note how the shadow follows the
// form of the layer sizes.
// TODO: eventually, we won't even need the shadow of the real data
// TODO: for now this is rectangular since the matrix constructor assumes a rectangular matrix. but
//  i should really make this have any variation of sizes.
layer_data = [0, 0; 0, 0; 0, 0; 0, 0; 0, 0; 0, 0]
// each element is a layer's height, excl. the input layer
layer_sizes  = [2, 2, 2, 2, 2, 2]
// number of layers, excl. the input layer
layer_amount = 5
// the learning rate
learning_rate = 0.005
// delta h with which to test slope
h = 0.00001

print("starting cost: ")
gen = cost(weights, biases, layer_data, layer_sizes, layer_amount, 1)
print(gen)
print("\n")

gens = 1
while gens <= 100 {
  i = 1
  while i <= layer_amount {
    // 20 weights
    j = 1
    while j <= 2 {
      k = 1
      while k <= 10 {
        gen  = cost(weights, biases, layer_data, layer_sizes, layer_amount, 0)
        weights[j, k] = weights[j, k] + h
        gen' = cost(weights, biases, layer_data, layer_sizes, layer_amount, 0)
        weights[j, k] = weights[j, k] - h

        delta = gen - gen'
        slope = delta / h
        weights[j, k] = weights[j, k] + slope * learning_rate

        k = k + 1
      }
      j = j + 1
    }

    // 20 biases (?)
    j = 1
    while j <= 2 {
      k = 1
      while k <= 10 {
        gen  = cost(weights, biases, layer_data, layer_sizes, layer_amount, 0)
        biases[j, k] = biases[j, k] + h
        gen' = cost(weights, biases, layer_data, layer_sizes, layer_amount, 0)
        biases[j, k] = biases[j, k] - h

        delta = gen - gen'
        slope = delta / h
        weights[j, k] = weights[j, k] + slope * learning_rate

        k = k + 1
      }
      j = j + 1
    }

    i = i + 1
  }

  gen  = cost(weights, biases, layer_data, layer_sizes, layer_amount, 1)
  print("current cost: ")
  print(gen)
  print("\n")

  gens = gens + 1
}